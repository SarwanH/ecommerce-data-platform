# ðŸ  E-Commerce Data Platform

[![CI Pipeline](https://github.com/SarwanH/ecommerce-data-platform/actions/workflows/ci.yml/badge.svg)](https://github.com/SarwanH/ecommerce-data-platform/actions/workflows/ci.yml)
[![Python 3.11](https://img.shields.io/badge/python-3.11-blue.svg)](https://www.python.org/downloads/)
[![dbt](https://img.shields.io/badge/dbt-1.7.4-FF694B.svg)](https://www.getdbt.com/)
[![DuckDB](https://img.shields.io/badge/DuckDB-0.9.2-FEF000.svg)](https://duckdb.org/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

> **An end-to-end data engineering platform powering e-commerce analytics, demand forecasting, and inventory optimization for a home improvement retail environment.**

This project demonstrates production-grade data engineering practices including ETL pipeline development, dimensional modeling, data quality validation, and workflow orchestrationâ€”built with the modern data stack.

---

## ðŸŽ¯ Business Context

### The Problem

Home improvement retailers face complex data challenges:
- **Inventory Management**: Balancing stock levels across 2,000+ stores with 35,000+ SKUs
- **Demand Forecasting**: Predicting seasonal demand for products (e.g., lawn equipment in spring, heating supplies in winter)
- **Omnichannel Analytics**: Tracking customer journeys across online, in-store, and mobile channels
- **Supply Chain Optimization**: Coordinating between retail stores, distribution centers, and fulfillment centers

### The Solution

This platform provides the data infrastructure to:

| Business Need | Data Solution |
|--------------|---------------|
| Inventory Optimization | Daily inventory snapshots with stock status classification and days-of-supply calculations |
| Demand Forecasting | Aggregated sales data with lag features and rolling averages for ML model input |
| Customer Analytics | RFM (Recency, Frequency, Monetary) segmentation and lifetime value calculations |
| Channel Performance | Cross-channel attribution with fulfillment type analysis (BOPIS, Ship-to-Home, In-Store) |

---

## ðŸ— Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                           DATA ARCHITECTURE                                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                              â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚
â”‚   â”‚   SOURCES    â”‚     â”‚  INGESTION   â”‚     â”‚   STORAGE    â”‚                â”‚
â”‚   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤     â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤     â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤                â”‚
â”‚   â”‚ â€¢ POS System â”‚â”€â”€â”€â”€â–¶â”‚   Python     â”‚â”€â”€â”€â”€â–¶â”‚  Data Lake   â”‚                â”‚
â”‚   â”‚ â€¢ E-Commerce â”‚     â”‚  Scripts     â”‚     â”‚  (Parquet)   â”‚                â”‚
â”‚   â”‚ â€¢ Inventory  â”‚     â”‚              â”‚     â”‚              â”‚                â”‚
â”‚   â”‚ â€¢ Clickstreamâ”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                 â”‚                         â”‚
â”‚                                                    â–¼                         â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚   â”‚                        TRANSFORMATION (dbt)                          â”‚   â”‚
â”‚   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤   â”‚
â”‚   â”‚                                                                      â”‚   â”‚
â”‚   â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚   â”‚
â”‚   â”‚   â”‚  RAW    â”‚â”€â”€â”€â”€â”€â–¶â”‚   STAGING    â”‚â”€â”€â”€â”€â”€â–¶â”‚       MARTS         â”‚    â”‚   â”‚
â”‚   â”‚   â”‚         â”‚      â”‚              â”‚      â”‚                     â”‚    â”‚   â”‚
â”‚   â”‚   â”‚raw.     â”‚      â”‚stg_products  â”‚      â”‚ fact_sales          â”‚    â”‚   â”‚
â”‚   â”‚   â”‚products â”‚      â”‚stg_stores    â”‚      â”‚ dim_product (SCD2)  â”‚    â”‚   â”‚
â”‚   â”‚   â”‚stores   â”‚      â”‚stg_customers â”‚      â”‚ dim_customer        â”‚    â”‚   â”‚
â”‚   â”‚   â”‚etc...   â”‚      â”‚stg_txns      â”‚      â”‚ dim_store           â”‚    â”‚   â”‚
â”‚   â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚ dim_date            â”‚    â”‚   â”‚
â”‚   â”‚                                          â”‚ mart_demand_forecastâ”‚    â”‚   â”‚
â”‚   â”‚                                          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚   â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                    â”‚                         â”‚
â”‚                                                    â–¼                         â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚
â”‚   â”‚    QUALITY   â”‚     â”‚ ORCHESTRATE  â”‚     â”‚   CONSUME    â”‚                â”‚
â”‚   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤     â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤     â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤                â”‚
â”‚   â”‚    Great     â”‚     â”‚   Apache     â”‚     â”‚ â€¢ Dashboards â”‚                â”‚
â”‚   â”‚ Expectations â”‚     â”‚   Airflow    â”‚     â”‚ â€¢ ML Models  â”‚                â”‚
â”‚   â”‚              â”‚     â”‚              â”‚     â”‚ â€¢ Analytics  â”‚                â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚
â”‚                                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Design Principles

1. **Medallion Architecture**: Raw â†’ Staging â†’ Marts layers for clear data lineage
2. **Idempotent Pipelines**: All transformations can be safely re-run
3. **Schema-on-Read**: Parquet files in the data lake preserve full fidelity
4. **Dimensional Modeling**: Star schema optimized for analytical queries
5. **Infrastructure as Code**: All configurations version-controlled

---

## ðŸ›  Tech Stack

| Layer | Technology | Purpose |
|-------|------------|---------|
| **Languages** | Python 3.11, SQL | Core development |
| **Data Warehouse** | DuckDB (dev) / BigQuery (prod) | Analytical database |
| **Transformation** | dbt Core 1.7 | SQL-based transformations with testing |
| **Orchestration** | Apache Airflow 2.8 | Pipeline scheduling and monitoring |
| **Data Quality** | Great Expectations | Automated data validation |
| **Data Generation** | Faker, NumPy, Pandas | Realistic test data creation |
| **Storage Format** | Apache Parquet | Columnar storage for analytics |
| **CI/CD** | GitHub Actions | Automated testing and deployment |
| **Containerization** | Docker, Docker Compose | Reproducible environments |

### Why These Technologies?

- **DuckDB**: Embedded analytical database with BigQuery-compatible SQLâ€”perfect for local development that mirrors production
- **dbt**: Industry-standard transformation layer with built-in testing, documentation, and lineage
- **Airflow**: Production-proven orchestrator used by companies like Airbnb, Lyft, and major retailers
- **Parquet**: Columnar format with 10x compression vs CSV, predicate pushdown for fast queries

---

## ðŸ“ Project Structure

```
ecommerce-data-platform/
â”‚
â”œâ”€â”€ ðŸ“‚ src/                          # Source code
â”‚   â”œâ”€â”€ ingestion/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ generate_data.py         # Synthetic data generator
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ schemas.py               # Data class definitions
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â””â”€â”€ database.py              # DuckDB connection utilities
â”‚
â”œâ”€â”€ ðŸ“‚ dbt/                          # dbt transformation project
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ staging/                 # Clean, typed source data
â”‚   â”‚   â”‚   â”œâ”€â”€ stg_products.sql
â”‚   â”‚   â”‚   â”œâ”€â”€ stg_stores.sql
â”‚   â”‚   â”‚   â”œâ”€â”€ stg_customers.sql
â”‚   â”‚   â”‚   â”œâ”€â”€ stg_transactions.sql
â”‚   â”‚   â”‚   â”œâ”€â”€ stg_inventory_snapshots.sql
â”‚   â”‚   â”‚   â””â”€â”€ stg_page_views.sql
â”‚   â”‚   â”œâ”€â”€ intermediate/            # Business logic layer
â”‚   â”‚   â”‚   â”œâ”€â”€ int_daily_sales.sql
â”‚   â”‚   â”‚   â”œâ”€â”€ int_product_performance.sql
â”‚   â”‚   â”‚   â””â”€â”€ int_customer_metrics.sql
â”‚   â”‚   â””â”€â”€ marts/                   # Consumption-ready models
â”‚   â”‚       â”œâ”€â”€ fact_sales.sql
â”‚   â”‚       â”œâ”€â”€ dim_product.sql
â”‚   â”‚       â”œâ”€â”€ dim_customer.sql
â”‚   â”‚       â”œâ”€â”€ dim_store.sql
â”‚   â”‚       â”œâ”€â”€ dim_date.sql
â”‚   â”‚       â””â”€â”€ mart_demand_forecast_input.sql
â”‚   â”œâ”€â”€ seeds/                       # Static reference data
â”‚   â”œâ”€â”€ tests/                       # Custom data tests
â”‚   â”œâ”€â”€ macros/                      # Reusable SQL functions
â”‚   â”œâ”€â”€ dbt_project.yml              # dbt configuration
â”‚   â””â”€â”€ packages.yml                 # dbt package dependencies
â”‚
â”œâ”€â”€ ðŸ“‚ airflow/                      # Pipeline orchestration
â”‚   â””â”€â”€ dags/
â”‚       â””â”€â”€ ecommerce_pipeline.py    # Main ETL DAG
â”‚
â”œâ”€â”€ ðŸ“‚ data/                         # Data storage
â”‚   â””â”€â”€ sample/                      # Sample parquet files
â”‚       â”œâ”€â”€ products.parquet
â”‚       â”œâ”€â”€ stores.parquet
â”‚       â”œâ”€â”€ customers.parquet
â”‚       â”œâ”€â”€ transactions.parquet
â”‚       â”œâ”€â”€ inventory_snapshots.parquet
â”‚       â””â”€â”€ page_views.parquet
â”‚
â”œâ”€â”€ ðŸ“‚ great_expectations/           # Data quality
â”‚   â””â”€â”€ expectations/
â”‚       â””â”€â”€ transactions_suite.json  # Validation rules
â”‚
â”œâ”€â”€ ðŸ“‚ tests/                        # Python tests
â”‚   â””â”€â”€ unit/
â”‚       â””â”€â”€ test_data_generation.py
â”‚
â”œâ”€â”€ ðŸ“‚ docs/                         # Documentation
â”‚   â”œâ”€â”€ architecture.md              # Design decisions (ADRs)
â”‚   â””â”€â”€ data-dictionary.md           # Business definitions
â”‚
â”œâ”€â”€ ðŸ“‚ docker/                       # Container configs
â”‚   â””â”€â”€ docker-compose.yml           # Airflow stack
â”‚
â”œâ”€â”€ ðŸ“‚ .github/workflows/            # CI/CD
â”‚   â””â”€â”€ ci.yml                       # GitHub Actions pipeline
â”‚
â”œâ”€â”€ requirements.txt                 # Python dependencies
â”œâ”€â”€ .gitignore
â””â”€â”€ README.md
```

---

## ðŸ“Š Data Model

### Entity Relationship Diagram

```
                            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                            â”‚    dim_date     â”‚
                            â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
                            â”‚ date_key (PK)   â”‚
                            â”‚ year            â”‚
                            â”‚ quarter         â”‚
                            â”‚ month           â”‚
                            â”‚ week_of_year    â”‚
                            â”‚ day_of_week     â”‚
                            â”‚ is_weekend      â”‚
                            â”‚ fiscal_year     â”‚
                            â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                     â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   dim_product   â”‚         â”‚   fact_sales   â”‚         â”‚  dim_customer   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤         â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤         â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ product_key(PK) â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”‚ transaction_id â”‚â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚ customer_id(PK) â”‚
â”‚ product_id      â”‚         â”‚ order_id       â”‚         â”‚ customer_type   â”‚
â”‚ sku             â”‚         â”‚ customer_id(FK)â”‚         â”‚ loyalty_tier    â”‚
â”‚ product_name    â”‚         â”‚ product_id(FK) â”‚         â”‚ lifetime_value  â”‚
â”‚ category        â”‚         â”‚ store_id (FK)  â”‚         â”‚ first_order_dateâ”‚
â”‚ subcategory     â”‚         â”‚ transaction_dt â”‚         â”‚ last_order_date â”‚
â”‚ brand           â”‚         â”‚ quantity       â”‚         â”‚ purchase_segmentâ”‚
â”‚ unit_price      â”‚         â”‚ unit_price     â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”‚ unit_cost       â”‚         â”‚ discount_amt   â”‚
â”‚ margin_pct      â”‚         â”‚ total_amount   â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ is_current(SCD2)â”‚         â”‚ order_status   â”‚         â”‚   dim_store     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚ channel        â”‚         â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
                            â”‚ fulfillment    â”‚â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚ store_id (PK)   â”‚
                            â”‚ line_cost      â”‚         â”‚ store_name      â”‚
                            â”‚ line_profit    â”‚         â”‚ store_type      â”‚
                            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚ city            â”‚
                                                       â”‚ state           â”‚
                                                       â”‚ region          â”‚
                                                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Key Metrics Calculated

| Metric | Description | Business Use |
|--------|-------------|--------------|
| `margin_pct` | (price - cost) / price Ã— 100 | Product profitability analysis |
| `line_profit` | Revenue - COGS per line item | Transaction-level P&L |
| `lifetime_value` | Sum of all customer purchases | Customer segmentation |
| `days_of_supply` | Inventory / avg daily sales | Stock-out prevention |
| `purchase_segment` | RFM-based classification | Marketing targeting |

---

## ðŸš€ Getting Started

### Prerequisites

- Python 3.9+ (3.11 recommended)
- Git
- 4GB+ available RAM (for data generation)

### Installation

```bash
# 1. Clone the repository
git clone https://github.com/SarwanH/ecommerce-data-platform.git
cd ecommerce-data-platform

# 2. Create and activate virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: .\venv\Scripts\activate

# 3. Install dependencies
pip install --upgrade pip
pip install -r requirements.txt

# 4. Configure dbt profile (one-time setup)
mkdir -p ~/.dbt
cat > ~/.dbt/profiles.yml << 'EOF'
home_depot_analytics:
  target: dev
  outputs:
    dev:
      type: duckdb
      path: './data/warehouse.duckdb'
      schema: analytics
EOF
```

---

## ðŸ’» Usage

### Generate Sample Data

```bash
# Generate 30 days of sample data (default)
python src/ingestion/generate_data.py

# Generate custom date range
python src/ingestion/generate_data.py --days 90 --output data/sample

# Output:
# âœ“ 500 products
# âœ“ 50 stores
# âœ“ 10,000 customers
# âœ“ ~150,000 transactions
# âœ“ ~300,000 inventory records
# âœ“ ~1,500,000 page views
```

### Initialize Data Warehouse

```bash
# Load parquet files into DuckDB
python src/utils/database.py

# Verify tables
python -c "
from src.utils.database import execute_query
print(execute_query('SELECT table_name FROM information_schema.tables WHERE table_schema = \'raw\''))
"
```

### Run dbt Transformations

```bash
cd dbt

# Install dbt packages
dbt deps

# Validate connection
dbt debug

# Run all models
dbt run

# Run specific layer
dbt run --select staging
dbt run --select marts

# Run with full refresh (rebuilds incremental models)
dbt run --full-refresh

# Generate documentation
dbt docs generate
dbt docs serve  # Opens browser at localhost:8080
```

### Query the Data

```python
from src.utils.database import execute_query

# Top 10 products by revenue
query = """
SELECT 
    p.product_name,
    p.category,
    SUM(f.total_amount) as revenue,
    SUM(f.line_profit) as profit
FROM marts.fact_sales f
JOIN marts.dim_product p ON f.product_id = p.product_id
WHERE f.order_status = 'delivered'
GROUP BY 1, 2
ORDER BY revenue DESC
LIMIT 10
"""
print(execute_query(query))
```

---

## âœ… Data Quality

### Great Expectations Suite

The `transactions_suite.json` validates:

| Expectation | Rule |
|-------------|------|
| `expect_column_to_exist` | Required columns present |
| `expect_column_values_to_be_unique` | `transaction_id` uniqueness |
| `expect_column_values_to_not_be_null` | Critical fields populated |
| `expect_column_values_to_be_between` | `quantity` between 1-1000 |
| `expect_column_values_to_be_in_set` | `channel` in ['online', 'in_store', 'app'] |

### dbt Tests

```yaml
# Example from schema.yml
models:
  - name: fact_sales
    columns:
      - name: transaction_id
        tests:
          - unique
          - not_null
      - name: order_status
        tests:
          - accepted_values:
              values: ['pending', 'confirmed', 'shipped', 'delivered', 'cancelled', 'returned']
```

Run tests:
```bash
cd dbt
dbt test
```

---

## â° Pipeline Orchestration

### Airflow DAG Overview

```
ecommerce_data_pipeline (Daily @ 6:00 AM UTC)
â”‚
â”œâ”€â”€ ðŸ“¥ ingestion (TaskGroup)
â”‚   â”œâ”€â”€ extract_transactions
â”‚   â”œâ”€â”€ extract_inventory
â”‚   â””â”€â”€ extract_clickstream
â”‚
â”œâ”€â”€ âœ… data_quality (TaskGroup)
â”‚   â””â”€â”€ run_quality_checks
â”‚
â”œâ”€â”€ ðŸ”„ dbt_transformations (TaskGroup)
â”‚   â”œâ”€â”€ dbt_deps
â”‚   â”œâ”€â”€ dbt_run_staging
â”‚   â”œâ”€â”€ dbt_run_intermediate
â”‚   â”œâ”€â”€ dbt_run_marts
â”‚   â””â”€â”€ dbt_test
â”‚
â”œâ”€â”€ ðŸ“Š analytics (TaskGroup)
â”‚   â”œâ”€â”€ generate_forecast_features
â”‚   â””â”€â”€ update_dashboards
â”‚
â””â”€â”€ ðŸ“§ notify_completion
```

### Running Airflow Locally

```bash
cd docker
docker-compose up -d

# Access UI at http://localhost:8080
# Username: admin
# Password: admin
```

---

## ðŸ§ª Testing

```bash
# Run all Python tests
pytest tests/ -v

# Run with coverage
pytest tests/ --cov=src --cov-report=html

# Run dbt tests
cd dbt && dbt test

# Run specific test file
pytest tests/unit/test_data_generation.py -v
```

### Test Coverage

| Module | Coverage |
|--------|----------|
| `src/ingestion` | Data generation functions |
| `src/utils` | Database connectivity |
| `dbt models` | Schema tests, uniqueness, referential integrity |

---

## ðŸ”® Future Enhancements

- [ ] **Streaming Ingestion**: Add Kafka/Kinesis for real-time clickstream
- [ ] **ML Pipeline**: Integrate demand forecasting model with Vertex AI
- [ ] **Dashboard**: Build Streamlit/Looker dashboard for KPIs
- [ ] **Data Contracts**: Implement schema registry for producer/consumer contracts
- [ ] **Cost Optimization**: Add BigQuery slot management and query optimization
- [ ] **Alerting**: PagerDuty/Slack integration for pipeline failures

---

## ðŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

<p align="center">
  <i>Built with â˜• and a passion for clean data pipelines</i>
</p>
